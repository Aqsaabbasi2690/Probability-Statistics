{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d54a5bb",
   "metadata": {},
   "source": [
    "## Quantifying Chances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b38cd7",
   "metadata": {},
   "source": [
    "Given Information:\n",
    "\n",
    "A disease is prevalent in 0.2% of a population\n",
    "We have a test that, given to a sick person, gives a true result 88% of the time\n",
    "Of all the people ever tested, 8% were positive\n",
    "\n",
    "Question:\n",
    "If Nazo is tested and test comes back positive, what are the chances that she actually has the disease?\n",
    "Options: 88%, 77%, 21%, 2%\n",
    "\n",
    "\n",
    "\n",
    "Answer: 2%\n",
    "Key Insight:\n",
    "Even with a positive test result, Nazo only has about a 2% chance of actually having the disease. This counterintuitive result occurs because:\n",
    "\n",
    "The disease is very rare (0.2% prevalence)\n",
    "Most positive tests are false positives due to the low base rate\n",
    "Even though the test is 88% accurate for sick people, the vast majority of the population is healthy\n",
    "\n",
    "This demonstrates the importance of considering base rates when interpreting medical test results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b154957e",
   "metadata": {},
   "source": [
    "## What is information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c1ad23",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\"Gooblede oompa googie\" less quantification\n",
    "\n",
    "\"I can speak Gibberish.\" more quantification\n",
    "\n",
    "This section appears to be making a point about how some statements convey more \"information\" than others, perhaps in terms of their meaning or how much they reduce uncertainty. \"Gooblede oompa googie\" is pure nonsense, offering no information, while \"I can speak Gibberish\" provides some information about the speaker's (unusual) ability, even if the content of \"Gibberish\" itself is meaningless. The term \"quantification\" suggests a focus on how information can be measured or evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338187f3",
   "metadata": {},
   "source": [
    "## Cryptanalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a377a9d",
   "metadata": {},
   "source": [
    " \"Let's say you're doing cryptanalysis\"\n",
    "\n",
    "Key 1 → \"Gooblede oompa googie\" → 29\n",
    "\n",
    "Key 2 → \"I can speak Gibberish.\" → 1058\n",
    "\n",
    "Bracket with \"Entropy\"\n",
    "\n",
    "\n",
    "The numbers \"29\" and \"1058\" are likely representing some measure of entropy for the respective \"keys\" or the output generated by those keys.\n",
    "\n",
    "\"Gooblede oompa googie\" (from the previous slide, identified as \"less quantification\") now has a low entropy value of \"29\". This suggests it's less random or predictable.\n",
    "\n",
    "\"I can speak Gibberish.\" (from the previous slide, identified as \"more quantification\") now has a significantly higher entropy value of \"1058\". This implies it's more random or unpredictable, and therefore, potentially a stronger key in cryptography."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34709d7f",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f882b0",
   "metadata": {},
   "source": [
    "Chance → Certainty/Uncertainty → Entropy\n",
    "\n",
    "Certainty/Uncertainty: Chance directly leads to how certain or uncertain we are about an outcome. If the chance of something is very high (e.g., 99.9%), we are more certain. If it's 50/50, we are highly uncertain.\n",
    "\n",
    "Entropy: This is the formal measure of that uncertainty or randomness. Higher uncertainty means higher entropy, and conversely, higher certainty (less randomness) means lower entropy. This reinforces the definition of entropy as a measure of unpredictability, as seen in the previous slide's \"Key 1\" and \"Key 2\" example.\n",
    "\n",
    "- Video streaming — file size\n",
    "\n",
    "- Password strength\n",
    "\n",
    "This is a direct carry-over from the initial slide, reaffirming that password strength is a practical application where the concept of information and entropy is crucial. A strong password has high entropy (is unpredictable), while a weak password has low entropy (is predictable).\n",
    "\n",
    "- Hoffman's codes — compression\n",
    "\n",
    " Huffman coding is a classic example of an entropy encoding algorithm used for lossless data compression. It assigns shorter codes to more frequent symbols and longer codes to less frequent symbols, thereby reducing the overall size of the data, based on the information content (or entropy) of the source.\n",
    "\n",
    " - Computer vision\n",
    "This suggests that understanding information is crucial in computer vision. In computer vision, images and videos are analyzed, and information (features, patterns, objects) needs to be extracted from them. Concepts like entropy can be used for image segmentation, feature selection, or even to measure the complexity of an image.\n",
    "\n",
    "- Machine learning — throw away data/information\n",
    "This is an interesting and somewhat counter-intuitive point. In machine learning, especially in tasks like dimensionality reduction or feature selection, models often \"throw away\" or discard data/information that is deemed irrelevant or redundant to focus on the most salient features for prediction. This process is about identifying which information is truly valuable and which can be safely ignored to improve efficiency or prevent overfitting. It highlights that not all data is equally informative.\n",
    "\n",
    "- Bits — foundations of all computation\n",
    "This is a fundamental statement. \"Bits\" (binary digits, 0s and 1s) are the basic unit of information in digital computing. All data, instructions, and operations in a computer are ultimately represented and processed using bits. This point reinforces that the concept of information, at its most granular level, underpins the entire field of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b161884",
   "metadata": {},
   "source": [
    "## Quantifying Chances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf0290d",
   "metadata": {},
   "source": [
    "The Coin Flip Scenario\n",
    "\n",
    "Flip a coin: This sets up the context.\n",
    "\n",
    "Q: \"What are the chances that it will land on its head?\" \n",
    "\n",
    "\"If I flip it 10 times, how many will be heads?\"\n",
    "\n",
    "H stands for \"Heads,\" and T stands for \"Tails.\"\n",
    "\n",
    "These are the possible outcomes or events that can occur when flipping a coin.\n",
    "\n",
    "+-----+-----+\n",
    "| 0.5 | 0.5 |  Ω ↪ universe\n",
    "+-----+-----+\n",
    "| H   | T   |  Ω = {H, T}\n",
    "+-----+-----+\n",
    "\n",
    "\n",
    "0.5 for H and 0.5 for T indicate that there's a 50% chance of getting Heads and a 50% chance of getting Tails.\n",
    "\n",
    "Ω (Omega) with an arrow pointing to \"universe\" and defined as Ω = {H, T} represents the sample space or universe of all possible outcomes. In the context of probability, the sample space is the set of all possible results of an experiment. For a single coin flip, the only possible results are Heads or Tails.\n",
    "\n",
    "\n",
    "Coin Flip Probabilities\n",
    "\n",
    "Chance of it being heads = P(H) = 0.5\n",
    "\n",
    "\" \" \" \" tails = P(T) = 0.5\n",
    "\n",
    "axioms is somthing we cannot prove but people agree on that.\n",
    "The two axioms of Probability:\n",
    "- must lie in : [0−1]\n",
    "0 represents an impossible event.\n",
    "\n",
    "1 represents a certain event.\n",
    "\n",
    "Probabilities cannot be negative or greater than 1. This aligns with the non-negativity axiom and the upper bound of probability.\n",
    "\n",
    "- Sum of all events must be 1\n",
    "This states the second (or third, depending on the axiom set) axiom: The sum of the probabilities of all possible outcomes in the sample space must be equal to 1. This is also known as the normalization axiom. If you consider all possible mutually exclusive events that make up the sample space, their probabilities must add up to 1, as one of them must occur.\n",
    "\n",
    "\n",
    "P(H) = 1.2 X\n",
    "This is an example of an invalid probability. The \"X\" clearly indicates that P(H) = 1.2 is incorrect because it violates the first axiom (probability must lie between 0 and 1).\n",
    "\n",
    "P(H) = 0.7 P(T) = 0.2\n",
    "This is an example that appears to follow the first axiom individually (0.7 and 0.2 are both between 0 and 1). However, if H and T are the only possible outcomes (as in a coin flip), then their sum 0.7 + 0.2 = 0.9 which is not equal to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab5484",
   "metadata": {},
   "source": [
    "## Now Flip coin twice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d38e1",
   "metadata": {},
   "source": [
    "{ HH, HT, TH, TT }\n",
    "\n",
    "The two flips are \"independent\".\n",
    "This is a crucial statement. In probability, two events are independent if the outcome of one does not affect the outcome of the other. For a fair coin, each flip is indeed independent of the previous or subsequent flips. This property is fundamental for calculating probabilities of joint events.\n",
    "\n",
    "P(HH)\n",
    "This is asking for the probability of getting \"Heads, Heads\".\n",
    "\n",
    "\"Both flips are heads\"\n",
    "This is a descriptive label for the event HH.\n",
    "\n",
    "+-------+-------+\n",
    "| 0.25  | 0.25  |\n",
    "+-------+-------+\n",
    "| HH    | HT    |\n",
    "+-------+-------+\n",
    "| 0.25  | 0.25  |\n",
    "+-------+-------+\n",
    "| TH    | TT    |\n",
    "+-------+-------+\n",
    "\n",
    "P(HH) = P(H) * P(H) = 0.5 * 0.5 = 0.25\n",
    "\n",
    "P(HT) = P(H) * P(T) = 0.5 * 0.5 = 0.25\n",
    "\n",
    "P(TH) = P(T) * P(H) = 0.5 * 0.5 = 0.25\n",
    "\n",
    "P(TT) = P(T) * P(T) = 0.5 * 0.5 = 0.25\n",
    "\n",
    "the sum of these probabilities (0.25 + 0.25 + 0.25 + 0.25 = 1.0), which adheres to the axiom that the sum of all event probabilities must be 1.\n",
    "\n",
    "\n",
    "P(favorable event) = \n",
    "fractext#ofwayseventcanoccurtext#ofallpossibleoutcomes\n",
    "This is the classical definition of probability, often used when all outcomes are equally likely.\n",
    "\n",
    "\"Favorable event\" refers to the specific outcome or set of outcomes you are interested in (e.g., HH).\n",
    "\n",
    "\"# of ways event can occur\" is the count of outcomes where the favorable event happens.\n",
    "\n",
    "\"# of all possible outcomes\" is the total number of outcomes in the sample space.\n",
    "\n",
    "P(HH) = 1/4 = 0.25.\n",
    "\n",
    "\n",
    "\"Any of the flips is a head.\" event\n",
    "A = { HH, HT, TH }\n",
    "P(A) = 3/4 = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f94253",
   "metadata": {},
   "source": [
    "## Alternate way | Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27710a51",
   "metadata": {},
   "source": [
    "\"Both flips are Heads\" (AND logic)\n",
    "First flip is H and second flip is H\n",
    "P(H) * P(H) \n",
    "= 0.5 * 0.5 = 0.25\n",
    "\n",
    "AND\n",
    "H O   H O | O\n",
    "H O   T 1 | O\n",
    "T 1   H O | O\n",
    "T 1   T 1 | 1\n",
    "\n",
    "\n",
    "\n",
    "\"Any of the flips is a head.\" (OR logic)\n",
    "First flip is H or second flip is H\n",
    "or\n",
    "H O   H O | 0\n",
    "H O   T 1 | 1\n",
    "T 1   H O | 1\n",
    "T 1   T 1 | 1\n",
    "P(H) * P(H) \n",
    "= 0.5 + 0.5 \n",
    "= 1\n",
    "\n",
    "\n",
    "Remove the overlap\n",
    "P(H) + P(H) → P(HH)\n",
    "= 0.5 + 0.5 = 0.25\n",
    "=0.75\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
